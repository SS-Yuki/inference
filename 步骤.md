> 1. inference.k 配置项
> 2. Generate():  
> 向Inference对象填充配置  
> 生成资源
> 3. Deployment 资源如何生成的？  
> workload 中的 Service 本身也有相应的 Generate 函数，workload 存放 Service 或 Job，二者均有相应的 Module 实现。  
> （最外层的 AppConfiguration 也存在配置项描述文件）
```py
# 分别是 Service 和 AppConfiguration
service = {oci = "oci://ghcr.io/kusionstack/service", tag = "0.1.0" }
kam = { git = "https://github.com/KusionStack/kam.git", tag = "0.2.0" }
```
> 4. 我们要在 Inference 中生成哪些资源？ yaml  
> Namespace  
> Deployment app  
> Deployment ollama  
> Service app  
> Service ollama  

```go
type Namespace struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty" protobuf:"bytes,1,opt,name=metadata"`
	Spec NamespaceSpec `json:"spec,omitempty" protobuf:"bytes,2,opt,name=spec"`
	Status NamespaceStatus `json:"status,omitempty" protobuf:"bytes,3,opt,name=status"`
}
type Deployment struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty" protobuf:"bytes,1,opt,name=metadata"`
	Spec DeploymentSpec `json:"spec,omitempty" protobuf:"bytes,2,opt,name=spec"`
	Status DeploymentStatus `json:"status,omitempty" protobuf:"bytes,3,opt,name=status"`
}
type Service struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty" protobuf:"bytes,1,opt,name=metadata"`
	Spec ServiceSpec `json:"spec,omitempty" protobuf:"bytes,2,opt,name=spec"`
	Status ServiceStatus `json:"status,omitempty" protobuf:"bytes,3,opt,name=status"`
}
```

```yaml
import kam.v1.app_configuration as ac
import service
import service.container as c
import inference.v1.inference

app: ac.AppConfiguration {
    # Declare the workload configurations. 
    workload: service.Service {
        containers: {
            myct: c.Container {
                image: "xxx/ai-app"
            }
        }
        replicas: 1
    }
    # Declare the kawesome module configurations. 
    accessories: {
        "inference": inference.Inference {
            model: "llama3"
            framework: "Ollama"
        }
        "network": n.Network {
            ports: [
                n.Port {
                    port: 80
                    public: True
                }
            ]
        }
    }
}
```

```yaml
# app image: ai-app
# Project: myproject
# Stack: dev
# App: app
# Container name: myct

apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: myproject
spec: {}
status: {}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/name: app
    app.kubernetes.io/part-of: myproject
  name: myproject-dev-app
  namespace: myproject
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: app
      app.kubernetes.io/part-of: myproject
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app.kubernetes.io/name: app
        app.kubernetes.io/part-of: myproject
    spec:
      containers:
        - image: xxx/ai-app
          name: myct
          resources: {}        
status: {}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/name: app
    app.kubernetes.io/part-of: myproject
    framework: ollama
  name: ollama-myproject-dev-app
  namespace: myproject
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: app
      app.kubernetes.io/part-of: myproject
      framework: ollama
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app.kubernetes.io/name: app
        app.kubernetes.io/part-of: myproject
        framework: ollama
    spec:
      initContainers:
        - name: init-ollama
          image: ollama/ollama:latest
          ports:
          command: ["/bin/sh"]
          args: ["-c", "ollama pull llama3"]
          volumeMounts:
          - name: ollama-storage
            mountPath: /root/.ollama
      containers:
        - name: ollama
          image: ollama/ollama:latest
          ports:
          - name: http
            containerPort: 11434
            protocol: TCP
          volumeMounts:
          - name: ollama-storage
            mountPath: /root/.ollama
      volumes:
        - name: ollama-storage
          emptyDir: {}    
status: {}
---
# 非本模块功能
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  name: myproject-dev-app
  namespace: myproject
spec:
  ports:
      - name: myproject-dev-app-80-tcp
        port: 80
        protocol: TCP
        targetPort: 80
  selector:
      app.kubernetes.io/name: app
      app.kubernetes.io/part-of: myproject
  type: ClusterIP
status:
  loadBalancer: {}
---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  name: ollama-myproject-dev-app
  namespace: myproject
spec:
  type: ClusterIP
  selector:
    name: ollama
  ports:
  - port: 80
    name: http
    targetPort: http
    protocol: TCP
```
> 5. 如何交互  
> svc_name:port
```go
envVars := []v1.EnvVar{
  {
    Name:  "KUSION_KAWESOME_RANDOM_PASSWORD",
    Value: modules.KusionPathDependency(resourceID, "result"),
  },
}
patcher := &apiv1.Patcher{
  Environments: envVars,
}
```
```yaml
kawesome: c.Container {
  image: "hashicorp/http-echo"
  env: {
    "ECHO_TEXT": "$(KUSION_KAWESOME_RANDOM_PASSWORD)"
  }
}

containers:
  - env:
    - name: KUSION_KAWESOME_RANDOM_PASSWORD
      value: $kusion_path.hashicorp:random:random_password:example-dev-kawesome.result
    - name: ECHO_TEXT
      value: $(KUSION_KAWESOME_RANDOM_PASSWORD)
  image: hashicorp/http-echo
  name: kawesome
  resources: {}
```
> 6. 如何测试

> - Deployment 部分？  